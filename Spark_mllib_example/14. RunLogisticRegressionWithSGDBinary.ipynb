{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetLogger( sc ):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetPath(sc):\n",
    "    global Path\n",
    "    if sc.master[0:5]==\"local\" :\n",
    "        Path=\"file:/home/hduser/pythonwork/PythonProject/\"\n",
    "    else:   \n",
    "        Path=\"hdfs://hdnn:8020/user/hduser/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果您要在cluster模式執行(hadoop yarn 或Spark Stand alone)，請依照書上說明，先上傳檔案至HDFS目錄\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(record):\n",
    "    label=(record[-1])\n",
    "    return float(label)\n",
    "\n",
    "def extract_features(field,categoriesMap,featureEnd):\n",
    "    categoryIdx = categoriesMap[field[3]]\n",
    "    categoryFeatures = np.zeros(len(categoriesMap))\n",
    "    categoryFeatures[categoryIdx] = 1\n",
    "    numericalFeatures=[convert_float(field)  for  field in field[4: featureEnd]]    \n",
    "    return  np.concatenate(( categoryFeatures, numericalFeatures))\n",
    "\n",
    "def convert_float(x):\n",
    "    return (0 if x==\"?\" else float(x))\n",
    "\n",
    "def PrepareData(sc): \n",
    "    #----------------------1.匯入並轉換資料-------------\n",
    "    print(\"開始匯入資料...\")\n",
    "    rawDataWithHeader = sc.textFile(Path+\"data/train.tsv\")\n",
    "    header = rawDataWithHeader.first() \n",
    "    rawData = rawDataWithHeader.filter(lambda x:x !=header)    \n",
    "    rData=rawData.map(lambda x: x.replace(\"\\\"\", \"\"))    \n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    print(\"共計：\" + str(lines.count()) + \"筆\")\n",
    "    #----------------------2.建立訓練評估所需資料 RDD[LabeledPoint]-------------\n",
    "    print(\"標準化之前：\")\n",
    "    categoriesMap = lines.map(lambda fields: fields[3]). \\\n",
    "                                        distinct().zipWithIndex().collectAsMap()\n",
    "    labelRDD = lines.map(lambda r:  extract_label(r))\n",
    "    featureRDD = lines.map(lambda r:  extract_features(r,categoriesMap,len(r) - 1))\n",
    "    for i in featureRDD.first():\n",
    "        print (str(i)+\",\"),\n",
    "    print(\"\")       \n",
    "    print(\"標準化之後：\")       \n",
    "    stdScaler = StandardScaler(withMean=True, withStd=True).fit(featureRDD)\n",
    "    ScalerFeatureRDD=stdScaler.transform(featureRDD)\n",
    "    for i in ScalerFeatureRDD.first():\n",
    "        print (str(i)+\",\"),\n",
    "    labelpoint=labelRDD.zip(ScalerFeatureRDD)\n",
    "    labelpointRDD=labelpoint.map(lambda r: LabeledPoint(r[0], r[1]))\n",
    "    #----------------------3.以隨機方式將資料分為3部份並且回傳-------------\n",
    "    (trainData, validationData, testData) = labelpointRDD.randomSplit([8, 1, 1])\n",
    "    print(\"將資料分trainData:\" + str(trainData.count()) + \n",
    "              \"   validationData:\" + str(validationData.count()) +\n",
    "              \"   testData:\" + str(testData.count()))\n",
    "    return (trainData, validationData, testData, categoriesMap) #回傳資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictData(sc,model,categoriesMap): \n",
    "    print(\"開始匯入資料...\")\n",
    "    rawDataWithHeader = sc.textFile(Path+\"data/test.tsv\")\n",
    "    header = rawDataWithHeader.first() \n",
    "    rawData = rawDataWithHeader.filter(lambda x:x !=header)    \n",
    "    rData=rawData.map(lambda x: x.replace(\"\\\"\", \"\"))    \n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    print(\"共計：\" + str(lines.count()) + \"筆\")\n",
    "    dataRDD = lines.map(lambda r:  ( r[0]  ,\n",
    "                            extract_features(r,categoriesMap,len(r) )))\n",
    "    DescDict = {\n",
    "           0: \"暫時性網頁(ephemeral)\",\n",
    "           1: \"長青網頁(evergreen)\"\n",
    "     }\n",
    "    for data in dataRDD.take(10):\n",
    "        predictResult = model.predict(data[1])\n",
    "        print(\" 網址：  \" +str(data[0])+\"\\n\" +\\\n",
    "                  \"             ==>預測:\"+ str(predictResult)+ \\\n",
    "                  \" 說明:\"+DescDict[predictResult] +\"\\n\")\n",
    "\n",
    "def evaluateModel(model, validationData):\n",
    "    score = model.predict(validationData.map(lambda p: p.features))\n",
    "    scoreAndLabels=score.zip(validationData.map(lambda p: p.label)).map(lambda t: (float(t[0]),float(t[1])))\n",
    "    metrics = BinaryClassificationMetrics(scoreAndLabels)\n",
    "    AUC=metrics.areaUnderROC\n",
    "    return( AUC)\n",
    "\n",
    "\n",
    "def trainEvaluateModel(trainData,validationData,\n",
    "                                        numIterations, stepSize, miniBatchFraction):\n",
    "    startTime = time()\n",
    "    model = LogisticRegressionWithSGD.train(trainData, \n",
    "                                        numIterations, stepSize, miniBatchFraction)\n",
    "    AUC = evaluateModel(model, validationData)\n",
    "    duration = time() - startTime\n",
    "    print(\"訓練評估：使用參數\" + \" numIterations=\"+str(numIterations) + \" stepSize=\"+str(stepSize) + \" miniBatchFraction=\"+str(miniBatchFraction) + \" 所需時間=\"+str(duration) + \" 結果AUC = \" + str(AUC))\n",
    "    return (AUC,duration, numIterations, stepSize, miniBatchFraction,model)\n",
    "\n",
    "\n",
    "def evalParameter(trainData, validationData, evalparm,\n",
    "                  numIterationsList, stepSizeList, miniBatchFractionList):\n",
    "    \n",
    "    metrics = [trainEvaluateModel(trainData, validationData,  \n",
    "                                numIterations,stepSize,  miniBatchFraction  ) \n",
    "                       for numIterations in numIterationsList\n",
    "                       for stepSize in stepSizeList  \n",
    "                       for miniBatchFraction in miniBatchFractionList ]\n",
    "    \n",
    "    if evalparm==\"numIterations\":\n",
    "        IndexList=numIterationsList[:]\n",
    "    elif evalparm==\"stepSize\":\n",
    "        IndexList=stepSizeList[:]\n",
    "    elif evalparm==\"miniBatchFraction\":\n",
    "        IndexList=miniBatchFractionList[:]\n",
    "    \n",
    "    df = pd.DataFrame(metrics,index=IndexList,\n",
    "            columns=['AUC', 'duration','numIterations', 'stepSize', 'miniBatchFraction','model'])\n",
    "    \n",
    "    showchart(df,evalparm,'AUC','duration',0.5,0.7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showchart(df,evalparm ,barData,lineData,yMin,yMax):\n",
    "    ax = df[barData].plot(kind='bar', title =evalparm,figsize=(10,6),legend=True, fontsize=12)\n",
    "    ax.set_xlabel(evalparm,fontsize=12)\n",
    "    ax.set_ylim([yMin,yMax])\n",
    "    ax.set_ylabel(barData,fontsize=12)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df[[lineData ]].values, linestyle='-', marker='o', linewidth=2.0,color='r')\n",
    "    plt.show()\n",
    "        \n",
    "def evalAllParameter(trainData, validationData, \n",
    "                     numIterationsList, stepSizeList, miniBatchFractionList):    \n",
    "    metrics = [trainEvaluateModel(trainData, validationData,  \n",
    "                            numIterations,stepSize,  miniBatchFraction  ) \n",
    "                      for numIterations in numIterationsList \n",
    "                      for stepSize in stepSizeList  \n",
    "                      for  miniBatchFraction in miniBatchFractionList ]\n",
    "    \n",
    "    Smetrics = sorted(metrics, key=lambda k: k[0], reverse=True)\n",
    "    bestParameter=Smetrics[0]\n",
    "    \n",
    "    print(\"調校後最佳參數：numIterations:\" + str(bestParameter[2]) + \n",
    "                                      \"  ,stepSize:\" + str(bestParameter[3]) + \n",
    "                                     \"  ,miniBatchFraction:\" + str(bestParameter[4])   + \n",
    "                                      \"  ,結果AUC = \" + str(bestParameter[0]))\n",
    "    \n",
    "    return bestParameter[5]\n",
    "\n",
    "def  parametersEval(trainData, validationData):\n",
    "    print(\"----- 評估numIterations參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"numIterations\", \n",
    "                              numIterationsList=[5, 15, 20, 60, 100],   \n",
    "                              stepSizeList=[10],  \n",
    "                              miniBatchFractionList=[1 ])  \n",
    "    print(\"----- 評估stepSize參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"stepSize\", \n",
    "                              numIterationsList=[100],                    \n",
    "                              stepSizeList=[10, 50, 100, 200],    \n",
    "                              miniBatchFractionList=[1])   \n",
    "    print(\"----- 評估miniBatchFraction參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"miniBatchFraction\", \n",
    "                              numIterationsList=[100],      \n",
    "                              stepSizeList =[100],        \n",
    "                              miniBatchFractionList=[0.5, 0.8, 1 ])\n",
    "\n",
    "\n",
    "\n",
    "def CreateSparkContext():\n",
    "    #sparkConf = SparkConf()                                                       \\\n",
    "    #                     .setAppName(\"LogisticRegressionWithSGD\")                         \\\n",
    "    #                     .set(\"spark.ui.showConsoleProgress\", \"false\") \n",
    "    #sc = SparkContext(conf = sparkConf)\n",
    "    print (\"master=\"+sc.master)    \n",
    "    SetLogger(sc)\n",
    "    SetPath(sc)\n",
    "    return (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogisticRegressionWithSGDBinary\n",
      "master=spark://spkma:7077\n",
      "==========資料準備階段===============\n",
      "開始匯入資料...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"RunLogisticRegressionWithSGDBinary\")\n",
    "    sc=CreateSparkContext()\n",
    "    print(\"==========資料準備階段===============\")\n",
    "    (trainData, validationData, testData, categoriesMap) =PrepareData(sc)\n",
    "    trainData.persist(); validationData.persist(); testData.persist()\n",
    "    print(\"==========訓練評估階段===============\")\n",
    "    (AUC,duration, numIterationsParm, stepSizeParm, miniBatchFractionParm,model)= \\\n",
    "          trainEvaluateModel(trainData, validationData, 15, 10, 0.5)\n",
    "    if (len(sys.argv) == 2) and (sys.argv[1]==\"-e\"):\n",
    "        parametersEval(trainData, validationData)\n",
    "    elif   (len(sys.argv) == 2) and (sys.argv[1]==\"-a\"): \n",
    "        print(\"-----所有參數訓練評估找出最好的參數組合---------\")  \n",
    "        model=evalAllParameter(trainData, validationData,\n",
    "                         [3, 5, 10,15], \n",
    "                         [10, 50, 100],\n",
    "                          [0.5, 0.8, 1 ])\n",
    "    print(\"==========測試階段===============\")\n",
    "    auc = evaluateModel(model, testData)\n",
    "    print(\"使用test Data測試最佳模型,結果 AUC:\" + str(auc))\n",
    "    print(\"==========預測資料===============\")\n",
    "    PredictData(sc, model, categoriesMap)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
