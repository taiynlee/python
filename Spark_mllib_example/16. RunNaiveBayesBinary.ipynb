{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetLogger( sc ):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)    \n",
    "\n",
    "def SetPath(sc):\n",
    "    global Path\n",
    "    if sc.master[0:5]==\"local\" :\n",
    "        Path=\"file:/home/hduser/pythonwork/PythonProject/\"\n",
    "    else:\n",
    "        Path=\"hdfs://hdnn:8020/user/hduser/\"\n",
    "#如果您要在cluster模式執行(hadoop yarn 或Spark Stand alone)，請依照書上說明，先上傳檔案至HDFS目錄\n",
    "\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "def extract_label(record):\n",
    "    label=(record[-1])\n",
    "    return float(label)\n",
    "\n",
    "def extract_features(field,categoriesMap,featureEnd):\n",
    "    categoryIdx = categoriesMap[field[3]]\n",
    "    categoryFeatures = np.zeros(len(categoriesMap))\n",
    "    categoryFeatures[categoryIdx] = 1\n",
    "    numericalFeatures=[convert_float(field)  for  field in field[4: featureEnd]]    \n",
    "    return  np.concatenate(( categoryFeatures, numericalFeatures))\n",
    "\n",
    "def convert_float(x):\n",
    "    ret=(0 if x==\"?\" else float(x))\n",
    "    return(0 if ret<0 else ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(sc): \n",
    "    #----------------------1.匯入並轉換資料-------------\n",
    "    print(\"開始匯入資料...\")\n",
    "    rawDataWithHeader = sc.textFile(Path+\"data/train.tsv\")\n",
    "    header = rawDataWithHeader.first() \n",
    "    rawData = rawDataWithHeader.filter(lambda x:x !=header)    \n",
    "    rData=rawData.map(lambda x: x.replace(\"\\\"\", \"\"))    \n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    print(\"共計：\" + str(lines.count()) + \"筆\")\n",
    "    #----------------------2.建立訓練評估所需資料 RDD[LabeledPoint]-------------\n",
    "    print (\"標準化之前：\")        \n",
    "    categoriesMap = lines.map(lambda fields: fields[3]). \\\n",
    "                                        distinct().zipWithIndex().collectAsMap()\n",
    "    labelRDD = lines.map(lambda r:  extract_label(r))\n",
    "    featureRDD = lines.map(lambda r:  extract_features(r,categoriesMap,len(r) - 1))\n",
    "    for i in featureRDD.first():\n",
    "        print (str(i)+\",\"),\n",
    "    print (\"\")       \n",
    "    \n",
    "    print (\"標準化之後：\")    \n",
    "    stdScaler = StandardScaler(withMean=False, withStd=True).fit(featureRDD)\n",
    "    ScalerFeatureRDD=stdScaler.transform(featureRDD)\n",
    "    for i in ScalerFeatureRDD.first():\n",
    "        print (str(i)+\",\"),        \n",
    "                \n",
    "    labelpoint=labelRDD.zip(ScalerFeatureRDD)\n",
    "    labelpointRDD=labelpoint.map(lambda r: LabeledPoint(r[0], r[1]))\n",
    "    \n",
    "    #----------------------3.以隨機方式將資料分為3部份並且回傳-------------\n",
    "    (trainData, validationData, testData) = labelpointRDD.randomSplit([8, 1, 1])\n",
    "    print(\"將資料分trainData:\" + str(trainData.count()) + \n",
    "              \"   validationData:\" + str(validationData.count()) +\n",
    "              \"   testData:\" + str(testData.count()))\n",
    "    return (trainData, validationData, testData, categoriesMap) #回傳資料\n",
    "\n",
    "    \n",
    "def PredictData(sc,model,categoriesMap): \n",
    "    print(\"開始匯入資料...\")\n",
    "    rawDataWithHeader = sc.textFile(Path+\"data/test.tsv\")\n",
    "    header = rawDataWithHeader.first() \n",
    "    rawData = rawDataWithHeader.filter(lambda x:x !=header)    \n",
    "    rData=rawData.map(lambda x: x.replace(\"\\\"\", \"\"))    \n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    print(\"共計：\" + str(lines.count()) + \"筆\")\n",
    "    dataRDD = lines.map(lambda r:  ( r[0]  ,\n",
    "                            extract_features(r,categoriesMap,len(r) )))\n",
    "    DescDict = {\n",
    "           0: \"暫時性網頁(ephemeral)\",\n",
    "           1: \"長青網頁(evergreen)\"\n",
    "     }\n",
    "    for data in dataRDD.take(10):\n",
    "        predictResult = model.predict(data[1])\n",
    "        print (\" 網址：  \" +str(data[0])+\"\\n\" + \"             ==>預測:\"+ str(predictResult)+ \" 說明:\"+DescDict[predictResult] +\"\\n\")\n",
    "\n",
    "def evaluateModel(model, validationData):\n",
    "    score = model.predict(validationData.map(lambda p: p.features))\n",
    "    scoreAndLabels=score.zip(validationData \\\n",
    "                                   .map(lambda p: p.label))  \\\n",
    "                                   .map(lambda t: (float(t[0]),float(t[1])) )\n",
    "    metrics = BinaryClassificationMetrics(scoreAndLabels)\n",
    "    AUC=metrics.areaUnderROC\n",
    "    return( AUC)\n",
    "\n",
    "\n",
    "def trainEvaluateModel(trainData,validationData,lambdaParam):\n",
    "    startTime = time()\n",
    "    model = NaiveBayes.train(trainData,   lambdaParam)\n",
    "    AUC = evaluateModel(model, validationData)\n",
    "    duration = time() - startTime\n",
    "    print    (\"訓練評估：使用參數\" + \" lambda=\"+str( lambdaParam) + \" 所需時間=\"+str(duration) + \" 結果AUC = \" + str(AUC)) \n",
    "    return (AUC,duration,  lambdaParam,model)\n",
    "\n",
    "\n",
    "def evalParameter(trainData, validationData, evalparm,\n",
    "                  lambdaParamList):\n",
    "    \n",
    "    metrics = [trainEvaluateModel(trainData, validationData,regParam ) \n",
    "                                  for regParam in  lambdaParamList]\n",
    "    \n",
    "    evalparm=\"lambdaParam\"\n",
    "    IndexList=lambdaParamList\n",
    "    \n",
    "    df = pd.DataFrame(metrics,index=IndexList,\n",
    "            columns=['AUC', 'duration',' lambdaParam','model'])\n",
    "    showchart(df,evalparm,'AUC','duration',0.5,0.7 )\n",
    "    \n",
    "def showchart(df,evalparm ,barData,lineData,yMin,yMax):\n",
    "    ax = df[barData].plot(kind='bar', title =evalparm,figsize=(10,6),legend=True, fontsize=12)\n",
    "    ax.set_xlabel(evalparm,fontsize=12)\n",
    "    ax.set_ylim([yMin,yMax])\n",
    "    ax.set_ylabel(barData,fontsize=12)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df[[lineData ]].values, linestyle='-', marker='o', linewidth=2.0,color='r')\n",
    "    plt.show()\n",
    "def evalAllParameter(training_RDD, validation_RDD, lambdaParamList):    \n",
    "    metrics = [trainEvaluateModel(trainData, validationData,  lambdaParam  ) \n",
    "                        for lambdaParam in lambdaParamList  ]\n",
    "    Smetrics = sorted(metrics, key=lambda k: k[0], reverse=True)\n",
    "    bestParameter=Smetrics[0]\n",
    "    \n",
    "    print(\"調校後最佳參數：lambdaParam:\" + str(bestParameter[2]) +  \n",
    "             \"  ,結果AUC = \" + str(bestParameter[0]))\n",
    "    return bestParameter[3]\n",
    "\n",
    "    \n",
    "def  parametersEval(trainData, validationData):\n",
    "    print(\"----- 評估lambda參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"lambdaParam\", \n",
    "            lambdaParamList=[1.0, 3.0, 5.0, 15.0, 25.0,30.0,35.0,40.0,45.0,50.0,60.0]) \n",
    "         \n",
    "\n",
    "\n",
    "def CreateSparkContext():\n",
    "    #sparkConf = SparkConf()                                                       \\\n",
    "    #                     .setAppName(\"RunNaiveBayesBinary\")                         \\\n",
    "    #                     .set(\"spark.ui.showConsoleProgress\", \"false\") \n",
    "    #sc = SparkContext(conf = sparkConf)\n",
    "    print (\"master=\"+sc.master)    \n",
    "    SetLogger(sc)\n",
    "    SetPath(sc)\n",
    "    return (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunNaiveBayesBinary\n",
      "master=spark://spkma:7077\n",
      "==========資料準備階段===============\n",
      "開始匯入資料...\n",
      "共計：7395筆\n",
      "標準化之前：\n",
      "1.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.789131,\n",
      "2.055555556,\n",
      "0.676470588,\n",
      "0.205882353,\n",
      "0.047058824,\n",
      "0.023529412,\n",
      "0.443783175,\n",
      "0.0,\n",
      "0.0,\n",
      "0.09077381,\n",
      "0.0,\n",
      "0.245831182,\n",
      "0.003883495,\n",
      "1.0,\n",
      "1.0,\n",
      "24.0,\n",
      "0.0,\n",
      "5424.0,\n",
      "170.0,\n",
      "8.0,\n",
      "0.152941176,\n",
      "0.079129575,\n",
      "\n",
      "標準化之後：\n",
      "3.08823447037,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "2.38210990583,\n",
      "0.238469258742,\n",
      "3.33017936032,\n",
      "1.40301544772,\n",
      "0.490307355432,\n",
      "0.323968347813,\n",
      "0.0777978299323,\n",
      "0.0,\n",
      "0.0,\n",
      "2.19018963312,\n",
      "0.0,\n",
      "4.68369735582,\n",
      "0.00206688652554,\n",
      "2.05550839928,\n",
      "2.11132763706,\n",
      "1.1768686024,\n",
      "0.0,\n",
      "0.611125152837,\n",
      "0.947253587774,\n",
      "2.4743966777,\n",
      "0.83444157063,\n",
      "0.998721352144,\n",
      "將資料分trainData:5958   validationData:734   testData:703\n",
      "==========訓練評估階段===============\n",
      "訓練評估：使用參數 lambda=60.0 所需時間=11.815093755722046 結果AUC = 0.6333377880731765\n",
      "==========測試階段===============\n",
      "使用test Data測試最佳模型,結果 AUC:0.6521097560975609\n",
      "==========預測資料===============\n",
      "開始匯入資料...\n",
      "共計：3171筆\n",
      " 網址：  http://www.lynnskitchenadventures.com/2009/04/homemade-enchilada-sauce.html\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://lolpics.se/18552-stun-grenade-ar\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.xcelerationfitness.com/treadmills.html\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.bloomberg.com/news/2012-02-06/syria-s-assad-deploys-tactics-of-father-to-crush-revolt-threatening-reign.html\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.wired.com/gadgetlab/2011/12/stem-turns-lemons-and-limes-into-juicy-atomizers/\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.latimes.com/health/boostershots/la-heb-fat-tax-denmark-20111013,0,2603132.story\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.howlifeworks.com/a/a?AG_ID=1186&cid=7340ci\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://romancingthestoveblog.wordpress.com/2010/01/13/sweet-potato-ravioli-with-lemon-sage-brown-butter-sauce/\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.funniez.net/Funny-Pictures/turn-men-down.html\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://youfellasleepwatchingadvd.com/\n",
      "             ==>預測:1.0 說明:長青網頁(evergreen)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"RunNaiveBayesBinary\")\n",
    "    sc=CreateSparkContext()\n",
    "    print(\"==========資料準備階段===============\")\n",
    "    (trainData, validationData, testData, categoriesMap) =PrepareData(sc)\n",
    "    trainData.persist(); validationData.persist(); testData.persist()\n",
    "    print(\"==========訓練評估階段===============\")\n",
    "    \n",
    "    (AUC,duration,  lambdaParam,model)= \\\n",
    "            trainEvaluateModel(trainData, validationData, 60.0)\n",
    "          \n",
    "    if (len(sys.argv) == 2) and (sys.argv[1]==\"-e\"):\n",
    "        parametersEval(trainData, validationData)\n",
    "    elif   (len(sys.argv) == 2) and (sys.argv[1]==\"-a\"): \n",
    "        print(\"-----所有參數訓練評估找出最好的參數組合---------\")  \n",
    "        model=evalAllParameter(trainData, validationData, \n",
    "                           [1.0, 3.0, 5.0, 15.0, 25.0,30.0,35.0,40.0,45.0,50.0,60.0])\n",
    "\n",
    "              \n",
    "    print(\"==========測試階段===============\")\n",
    "    auc = evaluateModel(model, testData)\n",
    "    print(\"使用test Data測試最佳模型,結果 AUC:\" + str(auc))\n",
    "    print(\"==========預測資料===============\")\n",
    "    PredictData(sc, model, categoriesMap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
