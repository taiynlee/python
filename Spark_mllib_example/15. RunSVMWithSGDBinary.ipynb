{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SetLogger( sc ):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)    \n",
    "\n",
    "def SetPath(sc):\n",
    "    global Path\n",
    "    if sc.master[0:5]==\"local\" :\n",
    "        Path=\"file:/home/hduser/pythonwork/PythonProject/\"\n",
    "    else:\n",
    "        Path=\"hdfs://hdnn:8020/user/hduser/\"\n",
    "#如果您要在cluster模式執行(hadoop yarn 或Spark Stand alone)，請依照書上說明，先上傳檔案至HDFS目錄\n",
    "        \n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n",
    "\n",
    "def extract_label(record):\n",
    "    label=(record[-1])\n",
    "    return float(label)\n",
    "\n",
    "def extract_features(field,categoriesMap,featureEnd):\n",
    "    categoryIdx = categoriesMap[field[3]]\n",
    "    categoryFeatures = np.zeros(len(categoriesMap))\n",
    "    categoryFeatures[categoryIdx] = 1\n",
    "    numericalFeatures=[convert_float(field)  for  field in field[4: featureEnd]]    \n",
    "    return  np.concatenate(( categoryFeatures, numericalFeatures))\n",
    "\n",
    "def convert_float(x):\n",
    "    return (0 if x==\"?\" else float(x))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(sc): \n",
    "    #----------------------1.匯入並轉換資料-------------\n",
    "    print(\"開始匯入資料...\")\n",
    "    rawDataWithHeader = sc.textFile(Path+\"data/train.tsv\")\n",
    "    header = rawDataWithHeader.first() \n",
    "    rawData = rawDataWithHeader.filter(lambda x:x !=header)    \n",
    "    rData=rawData.map(lambda x: x.replace(\"\\\"\", \"\"))    \n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    print(\"共計：\" + str(lines.count()) + \"筆\")\n",
    "    #----------------------2.建立訓練評估所需資料 RDD[LabeledPoint]-------------\n",
    "    print (\"標準化之前：\")\n",
    "    categoriesMap = lines.map(lambda fields: fields[3]). \\\n",
    "                                        distinct().zipWithIndex().collectAsMap()\n",
    "    labelRDD = lines.map(lambda r:  extract_label(r))\n",
    "    featureRDD = lines.map(lambda r:  extract_features(r,categoriesMap,len(r) - 1))\n",
    "\n",
    "    for i in featureRDD.first():\n",
    "        print((str(i)+\",\"))\n",
    "    print(\"\")\n",
    "    \n",
    "    stdScaler = StandardScaler(withMean=True, withStd=True).fit(featureRDD)\n",
    "    ScalerFeatureRDD=stdScaler.transform(featureRDD)\n",
    "    print(\"標準化之後：\")\n",
    "    for i in ScalerFeatureRDD.first():\n",
    "        print (str(i)+\",\"),        \n",
    "    labelpoint=labelRDD.zip(ScalerFeatureRDD)\n",
    "    labelpointRDD=labelpoint.map(lambda r: LabeledPoint(r[0], r[1]))\n",
    "\n",
    "    \n",
    "    #----------------------3.以隨機方式將資料分為3部份並且回傳-------------\n",
    "    (trainData, validationData, testData) = labelpointRDD.randomSplit([8, 1, 1])\n",
    "    print(\"將資料分trainData:\" + str(trainData.count()) + \n",
    "              \"   validationData:\" + str(validationData.count()) +\n",
    "              \"   testData:\" + str(testData.count()))\n",
    "    return (trainData, validationData, testData, categoriesMap) #回傳資料\n",
    "\n",
    "    \n",
    "def PredictData(sc,model,categoriesMap): \n",
    "    print(\"開始匯入資料...\")\n",
    "    rawDataWithHeader = sc.textFile(Path+\"data/test.tsv\")\n",
    "    header = rawDataWithHeader.first() \n",
    "    rawData = rawDataWithHeader.filter(lambda x:x !=header)    \n",
    "    rData=rawData.map(lambda x: x.replace(\"\\\"\", \"\"))    \n",
    "    lines = rData.map(lambda x: x.split(\"\\t\"))\n",
    "    print(\"共計：\" + str(lines.count()) + \"筆\")\n",
    "    dataRDD = lines.map(lambda r:  ( r[0]  ,\n",
    "                            extract_features(r,categoriesMap,len(r) )))\n",
    "    DescDict = {\n",
    "           0: \"暫時性網頁(ephemeral)\",\n",
    "           1: \"長青網頁(evergreen)\"\n",
    "     }\n",
    "    for data in dataRDD.take(10):\n",
    "        predictResult = model.predict(data[1])\n",
    "        print(\" 網址：  \" +str(data[0])+\"\\n\" + \"             ==>預測:\"+ str(predictResult)+ \" 說明:\"+DescDict[predictResult] +\"\\n\")\n",
    "\n",
    "def evaluateModel(model, validationData):\n",
    "    score = model.predict(validationData.map(lambda p: p.features))\n",
    "    scoreAndLabels=score.zip(validationData \\\n",
    "                                   .map(lambda p: p.label))  \\\n",
    "                                   .map(lambda t: (float(t[0]),float(t[1])) )\n",
    "    metrics = BinaryClassificationMetrics(scoreAndLabels)\n",
    "    AUC=metrics.areaUnderROC\n",
    "    return( AUC)\n",
    "\n",
    "\n",
    "def trainEvaluateModel(trainData,validationData,\n",
    "                                        numIterations, stepSize, regParam):\n",
    "    startTime = time()\n",
    "    model = SVMWithSGD.train(trainData, numIterations, stepSize, regParam)\n",
    "    AUC = evaluateModel(model, validationData)\n",
    "    duration = time() - startTime\n",
    "    print(\"訓練評估：使用參數\" + \" numIterations=\"+str(numIterations) + \" stepSize=\"+str(stepSize) + \" regParam=\"+str(regParam) + \" 所需時間=\"+str(duration) + \" 結果AUC = \" + str(AUC))\n",
    "    return (AUC,duration, numIterations, stepSize, regParam,model)\n",
    "\n",
    "\n",
    "def evalParameter(trainData, validationData, evalparm,\n",
    "                  numIterationsList, stepSizeList, regParamList):\n",
    "    \n",
    "    metrics = [trainEvaluateModel(trainData, validationData,  \n",
    "                                numIterations,stepSize,  regParam  ) \n",
    "                       for numIterations in numIterationsList\n",
    "                       for stepSize in stepSizeList  \n",
    "                       for regParam in regParamList ]\n",
    "    \n",
    "    if evalparm==\"numIterations\":\n",
    "        IndexList=numIterationsList[:]\n",
    "    elif evalparm==\"stepSize\":\n",
    "        IndexList=stepSizeList[:]\n",
    "    elif evalparm==\"regParam\":\n",
    "        IndexList=regParamList[:]\n",
    "    \n",
    "    df = pd.DataFrame(metrics,index=IndexList,\n",
    "            columns=['AUC', 'duration','numIterations', 'stepSize', 'regParam','model'])\n",
    "    showchart(df,evalparm,'AUC','duration',0.5,0.7 )\n",
    "    \n",
    "def showchart(df,evalparm ,barData,lineData,yMin,yMax):\n",
    "    ax = df[barData].plot(kind='bar', title =evalparm,figsize=(10,6),legend=True, fontsize=12)\n",
    "    ax.set_xlabel(evalparm,fontsize=12)\n",
    "    ax.set_ylim([yMin,yMax])\n",
    "    ax.set_ylabel(barData,fontsize=12)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df[[lineData ]].values, linestyle='-', marker='o', linewidth=2.0,color='r')\n",
    "    plt.show()\n",
    "    \n",
    "def evalAllParameter(trainData, validationData, \n",
    "                     numIterationsList, stepSizeList, regParamList):    \n",
    "    metrics = [trainEvaluateModel(trainData, validationData,  \n",
    "                            numIterations,stepSize,  regParam  ) \n",
    "                      for numIterations in numIterationsList \n",
    "                      for stepSize in stepSizeList  \n",
    "                      for  regParam in regParamList ]\n",
    "    \n",
    "    Smetrics = sorted(metrics, key=lambda k: k[0], reverse=True)\n",
    "    bestParameter=Smetrics[0]\n",
    "    \n",
    "    print(\"調校後最佳參數：numIterations:\" + str(bestParameter[2]) + \n",
    "                                      \"  ,stepSize:\" + str(bestParameter[3]) + \n",
    "                                     \"  ,regParam:\" + str(bestParameter[4])   + \n",
    "                                      \"  ,結果AUC = \" + str(bestParameter[0]))\n",
    "    \n",
    "    return bestParameter[5]\n",
    "\n",
    "def  parametersEval(trainData, validationData):\n",
    "    print(\"----- 評估numIterations參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"numIterations\", \n",
    "                              numIterationsList= [1, 3, 5, 15, 25],   \n",
    "                              stepSizeList=[100],  \n",
    "                              regParamList=[1 ])  \n",
    "    print(\"----- 評估stepSize參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"stepSize\", \n",
    "                              numIterationsList=[25],                    \n",
    "                              stepSizeList= [10, 50, 100, 200],    \n",
    "                              regParamList=[1])   \n",
    "    print(\"----- 評估regParam參數使用 ---------\")\n",
    "    evalParameter(trainData, validationData,\"regParam\", \n",
    "                              numIterationsList=[25],      \n",
    "                              stepSizeList =[100],        \n",
    "                              regParamList=[0.01, 0.1, 1 ])\n",
    "\n",
    "def CreateSparkContext():\n",
    "    #sparkConf = SparkConf()                                                       \\\n",
    "    #                     .setAppName(\"LogisticRegressionWithSGD\")                         \\\n",
    "    #                     .set(\"spark.ui.showConsoleProgress\", \"false\") \n",
    "    #sc = SparkContext(conf = sparkConf)\n",
    "    print (\"master=\"+sc.master)    \n",
    "    SetLogger(sc)\n",
    "    SetPath(sc)\n",
    "    return (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunSVMWithSGDBinary\n",
      "master=spark://spkma:7077\n",
      "==========資料準備階段===============\n",
      "開始匯入資料...\n",
      "共計：7395筆\n",
      "標準化之前：\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "1.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.0,\n",
      "0.789131,\n",
      "2.055555556,\n",
      "0.676470588,\n",
      "0.205882353,\n",
      "0.047058824,\n",
      "0.023529412,\n",
      "0.443783175,\n",
      "0.0,\n",
      "0.0,\n",
      "0.09077381,\n",
      "0.0,\n",
      "0.245831182,\n",
      "0.003883495,\n",
      "1.0,\n",
      "1.0,\n",
      "24.0,\n",
      "0.0,\n",
      "5424.0,\n",
      "170.0,\n",
      "8.0,\n",
      "0.152941176,\n",
      "0.079129575,\n",
      "\n",
      "標準化之後：\n",
      "-0.680752790425,\n",
      "-0.381813223243,\n",
      "-0.204182210579,\n",
      "2.72073665645,\n",
      "-0.220526884579,\n",
      "-0.0991499193088,\n",
      "-0.232727977095,\n",
      "-0.101894690972,\n",
      "-0.0648775723926,\n",
      "-0.0232621058984,\n",
      "-0.028494000387,\n",
      "-0.446421204794,\n",
      "-0.270999069693,\n",
      "-0.201654052319,\n",
      "1.1376473365,\n",
      "-0.0819355716929,\n",
      "1.02513981289,\n",
      "-0.0558635644254,\n",
      "-0.468893253129,\n",
      "-0.354305326308,\n",
      "-0.317535217236,\n",
      "0.33845079824,\n",
      "0.0,\n",
      "0.828822173315,\n",
      "-0.147268943346,\n",
      "0.229639823578,\n",
      "-0.141625969099,\n",
      "0.790238049918,\n",
      "0.717194729453,\n",
      "-0.297996816496,\n",
      "-0.20346257793,\n",
      "-0.0329672096969,\n",
      "-0.0487811297558,\n",
      "0.940069975117,\n",
      "-0.108698488525,\n",
      "-0.278820782314,\n",
      "將資料分trainData:5926   validationData:702   testData:767\n",
      "==========訓練評估階段===============\n",
      "訓練評估：使用參數 numIterations=3 stepSize=50 regParam=1 所需時間=17.102373838424683 結果AUC = 0.6495211038961038\n",
      "==========測試階段===============\n",
      "使用test Data測試最佳模型,結果 AUC:0.6465146650704686\n",
      "==========預測資料===============\n",
      "開始匯入資料...\n",
      "共計：3171筆\n",
      " 網址：  http://www.lynnskitchenadventures.com/2009/04/homemade-enchilada-sauce.html\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://lolpics.se/18552-stun-grenade-ar\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.xcelerationfitness.com/treadmills.html\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.bloomberg.com/news/2012-02-06/syria-s-assad-deploys-tactics-of-father-to-crush-revolt-threatening-reign.html\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.wired.com/gadgetlab/2011/12/stem-turns-lemons-and-limes-into-juicy-atomizers/\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.latimes.com/health/boostershots/la-heb-fat-tax-denmark-20111013,0,2603132.story\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.howlifeworks.com/a/a?AG_ID=1186&cid=7340ci\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://romancingthestoveblog.wordpress.com/2010/01/13/sweet-potato-ravioli-with-lemon-sage-brown-butter-sauce/\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://www.funniez.net/Funny-Pictures/turn-men-down.html\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n",
      " 網址：  http://youfellasleepwatchingadvd.com/\n",
      "             ==>預測:1 說明:長青網頁(evergreen)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"RunSVMWithSGDBinary\")\n",
    "    sc=CreateSparkContext()\n",
    "    print(\"==========資料準備階段===============\")\n",
    "    (trainData, validationData, testData, categoriesMap) =PrepareData(sc)\n",
    "    trainData.persist(); validationData.persist(); testData.persist()\n",
    "    print(\"==========訓練評估階段===============\")\n",
    "    (AUC,duration, numIterations, stepSize, regParam,model)= \\\n",
    "          trainEvaluateModel(trainData, validationData, 3, 50, 1)\n",
    "    if (len(sys.argv) == 2) and (sys.argv[1]==\"-e\"):\n",
    "        parametersEval(trainData, validationData)\n",
    "    elif   (len(sys.argv) == 2) and (sys.argv[1]==\"-a\"): \n",
    "        print(\"-----所有參數訓練評估找出最好的參數組合---------\")  \n",
    "        model=evalAllParameter(trainData, validationData,\n",
    "                        [1, 3, 5, 15, 25], \n",
    "                        [10, 50, 100, 200],\n",
    "                        [0.01, 0.1, 1 ])\n",
    "    print(\"==========測試階段===============\")\n",
    "    auc = evaluateModel(model, testData)\n",
    "    print(\"使用test Data測試最佳模型,結果 AUC:\" + str(auc))\n",
    "    print(\"==========預測資料===============\")\n",
    "    PredictData(sc, model, categoriesMap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
